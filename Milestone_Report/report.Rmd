---
title: "Milestone Report"
author: "nthehai01"
date: '2022-08-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = "")
```

## Capstone Introduction

The goal of this capstone is to mimic the experience of being a data scientist. As a practicing data scientist it is entirely common to get a messy data set, a vague question, and very little instruction on exactly how to analyze the data.

In this capstone we will be applying data science in the area of natural language processing. This capstone is inspired by the SwiftKey app, which predicts the next most probable words given a sentence while we are typing on the phones.

### Capstone tasks

This course will be separated into 8 different tasks that cover the range of activities encountered by a practicing data scientist. The tasks are:

-   Understanding the problem

-   Data acquisition and cleaning

-   Exploratory analysis

-   Statistical modeling

-   Predictive modeling

-   Creative exploration

-   Creating a data product

-   Creating a short slide deck pitching your product

### Capstone dataset

This is the training data to get you started that will be the basis for most of the capstone:

-   <https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip>

In this exercise, we will use the **English database** but may consider three other databases in German, Russian and Finnish.

## Milestone Report

**Note**: This report mainly focuses on task 2 (Exploratory analysis) and task 3 (Statistical modeling) and a little bit of task 1 (Data acquisition and cleaning) of the process.

### Getting and cleaning the data

#### Load the data

Load the English dataset:

```{r}
blogs <- readLines("../Data/en_US/en_US.blogs.txt", warn = FALSE, encoding = "UTF-8")
news <- readLines("../Data/en_US/en_US.news.txt", warn = FALSE, encoding = "UTF-8")
twitter <- readLines("../Data/en_US/en_US.twitter.txt", warn = FALSE, encoding = "UTF-8")
```

Remove all non-English words:

```{r}
blogs <-iconv(blogs,"latin1","ASCII",sub="")
news <-iconv(news,"latin1","ASCII",sub="")
twitter <-iconv(twitter,"latin1","ASCII",sub="")
```

#### Word counts and line counts

Word counts and line counts for each data set:

-   For *blogs* data set:

```{r}
library(stringr)

blogs.lines = length(blogs)
blogs.words = sum(str_count(string = blogs, pattern = '\\w+'))
```

-   For *news* data set:

```{r}
news.lines = length(news)
news.words = sum(str_count(string = news, pattern = '\\w+'))
```

-   For *twitter* data set:

```{r}
twitter.lines = length(twitter)
twitter.words = sum(str_count(string = twitter, pattern = '\\w+'))
```

Construct a data frame of counting:

```{r}
counting <- data.frame(LineCounts = c(blogs.lines, news.lines, twitter.lines),
                       WordCounts = c(blogs.words, news.words, twitter.words))
rownames(counting) <- c("blogs", "news", "twitter")

head(counting)
```

#### Get sub-sample data

To build a model, we don't need to use all the data at a time. We relatively use few randomly selected rows or chunks to represent for the entire data set. Since the data set is quite large, we only use a sub-sample data set of 1% of the whole data set.

```{r}
set.seed(42)
data <- c(sample(blogs, length(blogs)*0.01),
          sample(news, length(news)*0.01),
          sample(twitter, length(twitter)*0.01))
```

From now, we'll be using the sub-sample data set `data` instead of the large data set.

### Data preprocessing

Before performing EDA or building models, we first should do some data preprocessing to clean our data. There are few steps to get the data cleaner:

-   Creating corpus

-   Remove numbers

-   Remove punctuation

-   Strip whitespace

-   Lowercase

-   Remove stopwords

-   Stemming

```{r}
library(tm) # Text mining
library(NLP)
library(dplyr)

corpus <- VCorpus(VectorSource(data)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stemDocument)
```

### Exploratory Data Analysis

#### Understanding frequencies of words and word pairs using N-gram model

First, we define the n-gram tokenizer for uni-gram, bi-gram and tri-gram:

```{r}
library(RWeka)
uni.gram.tok <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
bi.gram.tok <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
tri.gram.tok <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
```

Next, we'll build the matrices of n-gram from our corpus:

```{r}
uni.gram.matrix <- TermDocumentMatrix(corpus, control=list(tokenize=uni.gram.tok))
bi.gram.matrix <- TermDocumentMatrix(corpus, control=list(tokenize=bi.gram.tok))
tri.gram.matrix <- TermDocumentMatrix(corpus, control=list(tokenize=tri.gram.tok))
```

The n-gram matrices above are actually the matrices with the rows indicating the words or pairs of words, the columns representing the indices of lines in our sub-sample data. The values of each matrix are the number of times a word or a pair of words appear in a corresponding line.

Finally, we'll build the matrices that represent the frequencies of words and word pairs and sort the value of occurrence descending. Since there are many words and word pairs to be processed (which take a long time), we only keep the words or word pairs that meet our threshold of frequencies (i.e. if the words or word pairs that less occur, we will omit them):

-   Uni-gram matrix with the threshold of 10 times:

```{r}
uni.gram.threshold <- findFreqTerms(uni.gram.matrix, lowfreq=10)

# sum of frequencies across the entire lines in the sub-sample
uni.gram <- rowSums(as.matrix(uni.gram.matrix[uni.gram.threshold,]))

# create data frame
uni.gram <- data.frame(Word=names(uni.gram), frequency=uni.gram)

head(uni.gram)
```

-   Bi-gram matrix with the threshold of 2 times:

```{r}
# only keep the words that more occur
bi.gram.threshold <- findFreqTerms(bi.gram.matrix, lowfreq=3)

# sum of frequencies across the entire lines in the sub-sample
bi.gram <- rowSums(as.matrix(bi.gram.matrix[bi.gram.threshold,]))

# create data frame
bi.gram <- data.frame(Word=names(bi.gram), frequency=bi.gram)

head(bi.gram)
```

-   Tri-gram matrix with the threshold of 2 times:

```{r}
# only keep the words that more occur
tri.gram.threshold <- findFreqTerms(tri.gram.matrix, lowfreq=2)

# sum of frequencies across the entire lines in the sub-sample
tri.gram <- rowSums(as.matrix(tri.gram.matrix[tri.gram.threshold,]))

# create data frame
tri.gram <- data.frame(Word=names(tri.gram), frequency=tri.gram)

head(tri.gram)
```

#### **Exploratory analysis**

-   Plot the word cloud of uni-gram word pairs:

```{r}
library(wordcloud)
wordcloud(words = uni.gram$Word, 
          freq = uni.gram$frequency, 
          min.freq = 10, 
          random.order = FALSE, 
          scale = c(3, 0.5), 
          colors = rainbow(3))
```

-   Plot the frequency distribution of uni-gram word pairs:

```{r}
library(ggplot2)
p <- ggplot(uni.gram, aes(x=frequency)) + 
  geom_histogram()
p
```

-   Plot the frequency distribution of bi-gram word pairs:

```{r}
library(ggplot2)
p <- ggplot(bi.gram, aes(x=frequency)) + 
  geom_histogram()
p
```

-   Plot the frequency distribution of tri-gram word pairs:

```{r}
p <- ggplot(tri.gram, aes(x=frequency)) + 
  geom_histogram()
p
```

### Save the n-gram matrices

Save the uni-gram, bi-gram and tri-gram matrices:

```{r}
write.csv(uni.gram, file="../Data/n-gram/uni-gram.csv", row.names=FALSE)
write.csv(bi.gram, file="../Data/n-gram/bi-gram.csv", row.names=FALSE)
write.csv(tri.gram, file="../Data/n-gram/tri-gram.csv", row.names=FALSE)
```
